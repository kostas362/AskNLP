{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruction import auto_reconstruct,spacy_reconstruct,spacy_auto_reconstruct,transformers_reconstruct\n",
    "from similarity import cosine_sim_embeddings, get_bert_embeddings, visualize_embeddings\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "reconstruct_pipeline = pipeline(\"text2text-generation\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "def split_into_sentences(text):\n",
    "    return re.split(r'(?<=[.!?]) +', text)\n",
    "#Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Ï€ÏÎ¿Ï‚ Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î®\n",
    "sentence1 = \"Thank your message to show our words to the doctor, as his next contract checking, to all of us.\"\n",
    "sentence2 = \"Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation.\"\n",
    "\n",
    "\n",
    "#Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ Ï„Î¿Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î¿ Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î®\n",
    "reconstructed_text1_auto = auto_reconstruct(sentence1)\n",
    "reconstructed_text2_auto = auto_reconstruct(sentence2)\n",
    "\n",
    "#ÎµÎºÏ„ÏÏ€Ï‰ÏƒÎ· Ï„Ï‰Î½ Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Ï‰Î½ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½\n",
    "print(\"Î‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 1:\")\n",
    "print(reconstructed_text1_auto)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 2:\")\n",
    "print(reconstructed_text2_auto)\n",
    "\n",
    "#ÎºÎµÎ¯Î¼ÎµÎ½Î± Ï€ÏÎ¿Ï‚ Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î®\n",
    "text1 = \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication\"\n",
    "text2 = \"During our final discuss, I told him about the new submission â€” the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didnâ€™t see that part final yet, or maybe I missed, I apologize if so. Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\"\n",
    "\n",
    "#Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½ Î¼Îµ T5\n",
    "reconstructed_text1_transformers = transformers_reconstruct(text1)\n",
    "reconstructed_text2_transformers = transformers_reconstruct(text2)\n",
    "\n",
    "#Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½ Î¼Îµ spacy\n",
    "reconstructed_text1_spacy = spacy_reconstruct(text1)\n",
    "reconstructed_text2_spacy = spacy_reconstruct(text2)\n",
    "\n",
    "#Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½ Î¼Îµ auto + spacy\n",
    "reconstructed_text1_Autospacy = spacy_auto_reconstruct(text1)\n",
    "reconstructed_text2_Autospacy = spacy_auto_reconstruct(text2)\n",
    "\n",
    "print(\"\\n ---Î‘Î½Î¬Î»Ï…ÏƒÎ· Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ Auto Reconstruct \")\n",
    "sentences = [sentence1, sentence2]\n",
    "\n",
    "#Î±Î½Î¬Î»Ï…ÏƒÎ· Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î´Î¹ÎºÎ¿Ï Î¼Î¿Ï… Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î¿Ï…\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    reconstructed = auto_reconstruct(sent)\n",
    "\n",
    "    print(f\"\\nğŸ”¹ Î ÏÏŒÏ„Î±ÏƒÎ· {i}:\")\n",
    "    print(f\"Original     : {sent}\")\n",
    "    print(f\"Reconstructed: {reconstructed}\")\n",
    "\n",
    "    if sent.strip() == reconstructed.strip():\n",
    "        print(\"Î§Ï‰ÏÎ¯Ï‚ Î±Î»Î»Î±Î³Î­Ï‚\")\n",
    "    elif len(reconstructed.split()) >= 3 and reconstructed[0].isupper():\n",
    "        print(\"Î‘Î»Î»Î±Î³Î¼Î­Î½Î· Î±Î»Î»Î¬ Ï€Î¹Î¸Î±Î½ÏÏ‚ ÏƒÏ‰ÏƒÏ„Î®\")\n",
    "    else:\n",
    "        print(\"Î Î¹Î¸Î±Î½Î® Î±ÏƒÏ…Î½Î¬ÏÏ„Î·Ï„Î· Î® Î±Ï„ÎµÎ»Î®Ï‚\")\n",
    "        \n",
    "#Î±Î½Î±Î»Ï…ÏƒÎ· Ï€ÏÎ¿Ï„Î±ÏƒÎµÏ‰Î½ transformers        \n",
    "print(\"\\n ---Î‘Î½Î¬Î»Ï…ÏƒÎ· Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ Transformers \")\n",
    "sentences = split_into_sentences(text1)\n",
    "\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    if len(sent.strip()) < 5:\n",
    "        continue  # Î±Î³Î½ÏŒÎ·ÏƒÎµ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÎ­Ï‚ \"Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚\"\n",
    "\n",
    "    reconstructed = transformers_reconstruct(sent)\n",
    "\n",
    "    print(f\"\\nğŸ”¹ Î ÏÏŒÏ„Î±ÏƒÎ· {i}:\")\n",
    "    print(f\"Original   : {sent}\")\n",
    "    print(f\"Reconstructed: {reconstructed}\")\n",
    "\n",
    "    # Î‘Ï€Î»Î® Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· Î±Î»Î»Î±Î³Î® Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÏ‰Î½\n",
    "    if sent.strip() == reconstructed.strip():\n",
    "        print(\"Î§Ï‰ÏÎ¯Ï‚ Î±Î»Î»Î±Î³Î­Ï‚\")\n",
    "    elif len(reconstructed.split()) >= 3 and reconstructed[0].isupper():\n",
    "        print(\"Î‘Î»Î»Î±Î³Î¼Î­Î½Î· Î±Î»Î»Î¬ Ï€Î¹Î¸Î±Î½ÏÏ‚ ÏƒÏ‰ÏƒÏ„Î®\")\n",
    "    else:\n",
    "        print(\"Î Î¹Î¸Î±Î½Î® Î±ÏƒÏ…Î½Î¬ÏÏ„Î·Ï„Î· Î® Î±Ï„ÎµÎ»Î®Ï‚\")\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 1 (Transformers):\")\n",
    "print(reconstructed_text1_transformers)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 2 (Transformers):\")\n",
    "print(reconstructed_text2_transformers)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 1 (spaCy):\")\n",
    "print(reconstructed_text1_spacy)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 2 (spaCy):\")\n",
    "print(reconstructed_text2_spacy)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 1 (auto+spaCy):\")\n",
    "print(reconstructed_text1_Autospacy)\n",
    "print(\"\\nÎ‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÎ¼Î­Î½Î¿ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 2 (auto+spaCy):\")\n",
    "print(reconstructed_text2_Autospacy)\n",
    "\n",
    "#Î»Î®ÏˆÎ· Ï„Ï‰Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÏ‰Î½ Î»Î­Î¾ÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î¿ bert\n",
    "embedding_text1 = get_bert_embeddings(text1)\n",
    "embedding_text2 = get_bert_embeddings(text2)\n",
    "\n",
    "embedding_reconstructed_text1_auto = get_bert_embeddings(reconstructed_text1_auto)\n",
    "embedding_reconstructed_text2_auto = get_bert_embeddings(reconstructed_text2_auto)\n",
    "\n",
    "embedding_reconstructed_text1_transformers = get_bert_embeddings(reconstructed_text1_transformers)\n",
    "embedding_reconstructed_text2_transformers = get_bert_embeddings(reconstructed_text2_transformers)\n",
    "\n",
    "embedding_reconstructed_text1_spacy = get_bert_embeddings(reconstructed_text1_spacy)\n",
    "embedding_reconstructed_text2_spacy = get_bert_embeddings(reconstructed_text2_spacy)\n",
    "\n",
    "embedding_reconstructed_text1_Autospacy = get_bert_embeddings(reconstructed_text1_Autospacy)\n",
    "embedding_reconstructed_text2_Autospacy = get_bert_embeddings(reconstructed_text2_Autospacy)\n",
    "\n",
    "\n",
    "#Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ cosine similarity Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î­Î¸Î¿Î´Î¿\n",
    "similarity1_auto = cosine_sim_embeddings(embedding_text1, embedding_reconstructed_text1_auto)\n",
    "similarity2_auto = cosine_sim_embeddings(embedding_text2, embedding_reconstructed_text2_auto)\n",
    "\n",
    "similarity1_transformers = cosine_sim_embeddings(embedding_text1, embedding_reconstructed_text1_transformers)\n",
    "similarity2_transformers = cosine_sim_embeddings(embedding_text2, embedding_reconstructed_text2_transformers)\n",
    "\n",
    "similarity1_spacy = cosine_sim_embeddings(embedding_text1, embedding_reconstructed_text1_spacy)\n",
    "similarity2_spacy = cosine_sim_embeddings(embedding_text2, embedding_reconstructed_text2_spacy)\n",
    "\n",
    "similarity1_Autospacy = cosine_sim_embeddings(embedding_text1, embedding_reconstructed_text1_Autospacy)\n",
    "similarity2_Autospacy = cosine_sim_embeddings(embedding_text2, embedding_reconstructed_text2_Autospacy)\n",
    "\n",
    "def compare_with_word2vec(original_text, reconstructed_text):\n",
    "\n",
    "    # Fallback tokenizer\n",
    "    def simple_tokenize(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.split()  # Î±Ï€Î»ÏŒ split Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î± ÎºÎµÎ½Î¬\n",
    "\n",
    "    tokens_orig = simple_tokenize(original_text)\n",
    "    tokens_recon = simple_tokenize(reconstructed_text)\n",
    "\n",
    "    model = Word2Vec([tokens_orig, tokens_recon], vector_size=100, window=5, min_count=1, workers=2)\n",
    "\n",
    "    def avg_vec(tokens):\n",
    "        vectors = [model.wv[t] for t in tokens if t in model.wv]\n",
    "        return sum(vectors) / len(vectors) if vectors else None\n",
    "\n",
    "    vec_orig = avg_vec(tokens_orig)\n",
    "    vec_recon = avg_vec(tokens_recon)\n",
    "\n",
    "    similarity = cosine_similarity([vec_orig], [vec_recon])[0][0] if vec_orig is not None and vec_recon is not None else 0\n",
    "\n",
    "    vocab_orig = set(tokens_orig)\n",
    "    vocab_recon = set(tokens_recon)\n",
    "    overlap_ratio = len(vocab_orig.intersection(vocab_recon)) / len(vocab_orig.union(vocab_recon))\n",
    "\n",
    "    return similarity, overlap_ratio\n",
    "\n",
    "#ÎµÎºÏ„ÏÏ€Ï‰ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ cosine similarity\n",
    "print(\"\\n--- Cosine Similarities Î¼Îµ BERT\")\n",
    "print(f\"Auto Reconstruct:\\n  Text1: {similarity1_auto:.4f}\\n  Text2: {similarity2_auto:.4f}\")\n",
    "print(f\"Transformers:\\n  Text1: {similarity1_transformers:.4f}\\n  Text2: {similarity2_transformers:.4f}\")\n",
    "print(f\"spaCy:\\n  Text1: {similarity1_spacy:.4f}\\n  Text2: {similarity2_spacy:.4f}\")\n",
    "print(f\"Auto + spaCy:\\n  Text1: {similarity1_Autospacy:.4f}\\n  Text2: {similarity2_Autospacy:.4f}\")\n",
    "\n",
    "#Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÏ‰Î½ Î»Î­Î¾ÎµÏ‰Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ PCA/t-SNE\n",
    "df = pd.DataFrame({\n",
    "    \"Method\": [\"Auto\",\"Transformers\", \"spaCy\",\"Auto + spaCy\"],\n",
    "    \"Cosine Similarity Text1\": [similarity1_auto, similarity1_transformers, similarity1_spacy,similarity1_Autospacy],\n",
    "    \"Cosine Similarity Text2\": [similarity2_auto, similarity1_transformers, similarity2_spacy,similarity2_Autospacy]\n",
    "})\n",
    "\n",
    "print(\"\\nÎ Î¯Î½Î±ÎºÎ±Ï‚ Similarities:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "pipelines = {\n",
    "    \"Auto\": reconstructed_text1_auto,\n",
    "    \"Transformers\": reconstructed_text1_transformers,\n",
    "    \"spaCy\": reconstructed_text1_spacy,\n",
    "    \"Auto + spaCy\": reconstructed_text1_Autospacy\n",
    "}\n",
    "\n",
    "print(\"\\n--- Word2Vec & Lexical Overlap per Pipeline (Text1)\")\n",
    "for name, reconstructed in pipelines.items():\n",
    "    sim, lex_overlap = compare_with_word2vec(text1, reconstructed)\n",
    "    print(f\"{name}:\\n  Word2Vec Cosine Similarity: {sim:.4f}\\n  Lexical Overlap (Jaccard): {lex_overlap:.4f}\")\n",
    "\n",
    "#Î³Î¹Î± Ï„Î·Î½ Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÎ½ Î»Î­Î¾ÎµÏ‰Î½\n",
    "visualize_embeddings([\n",
    "    embedding_reconstructed_text1_auto.squeeze().numpy(),\n",
    "    embedding_reconstructed_text2_auto.squeeze().numpy(),\n",
    "    embedding_reconstructed_text1_transformers.squeeze().numpy(),\n",
    "    embedding_reconstructed_text2_transformers.squeeze().numpy(),\n",
    "    embedding_reconstructed_text1_spacy.squeeze().numpy(),\n",
    "    embedding_reconstructed_text2_spacy.squeeze().numpy(),\n",
    "    embedding_reconstructed_text1_Autospacy.squeeze().numpy(),\n",
    "    embedding_reconstructed_text2_Autospacy.squeeze().numpy()\n",
    "])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
