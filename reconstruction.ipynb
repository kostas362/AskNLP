{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362028c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "\n",
    "#προτάσεις με αυτόματο που διαμόρφωσα\n",
    "def auto_reconstruct(text):\n",
    "    #kανόνες αυτόματου για ανακατασκευή\n",
    "    text = re.sub(r\"hope you too, to enjoy\", \"I hope you enjoy\", text)\n",
    "    text = re.sub(r\"Thank your message\", \"Thank you for your message\", text)\n",
    "    text = re.sub(r\"as his next contract checking\", \"regarding his upcoming contract review\", text)\n",
    "    text = re.sub(r\"although bit delay\", \"although there was a slight delay\", text)\n",
    "    text = re.sub(r\"to show our words to the doctor\", \"to forward our message to the doctor\", text)\n",
    "    text = re.sub(r\"for paper and cooperation\", \"for the paper and collaboration\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#μοντέλο αγγλικών\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#ανακατασκευη με spacy\n",
    "def spacy_reconstruct(text):\n",
    "    doc = nlp(text)\n",
    "    reconstructed_sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text for token in sent if not token.is_punct or token.text == '.']\n",
    "        sentence = ' '.join(tokens).strip()\n",
    "        reconstructed_sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(reconstructed_sentences)\n",
    "\n",
    "def spacy_auto_reconstruct(text):\n",
    "    #καλει το auto_reconstruct και μετα θα καλέσει και το spacy\n",
    "    text = auto_reconstruct(text)\n",
    "    \n",
    "    #κανει load το μοντέλο spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    reconstructed_sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text for token in sent if not token.is_punct or token.text == '.']\n",
    "        sentence = ' '.join(tokens).strip()\n",
    "        reconstructed_sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(reconstructed_sentences)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")\n",
    "\n",
    "#ανακατασκευη με transformers\n",
    "def transformers_reconstruct(text):\n",
    "    input_text = \"paraphrase: \" + text.strip()\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if not decoded or decoded.lower().startswith(\"paraphrase\") or decoded == text.strip():\n",
    "        return text.strip()\n",
    "    return decoded\n",
    "\n",
    "#σπαει το κειμενο σε chunks απ οτι θυμαμαι\n",
    "def chunk_and_paraphrase(text, max_chunk_words=45):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+max_chunk_words]) for i in range(0, len(words), max_chunk_words)]\n",
    "    full_output = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"\\n🔹 Paraphrasing chunk {idx+1}:\")\n",
    "        result = transformers_reconstruct(chunk)\n",
    "        print(f\"[{idx+1}] {result}\")\n",
    "        full_output.append(result)\n",
    "\n",
    "    return ' '.join(full_output)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
